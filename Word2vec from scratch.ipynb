{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec from Scratch\n",
    "Reinventing a wheel is usually an awesome way to learn something deeply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)  #N=16\n",
    "    X, Y = [], []\n",
    "    t = 0\n",
    "    for i in range(N): #i=0-15\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "#        print('i=',i,'t=',t)\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])  #the central words\n",
    "            Y.append(word_to_id[tokens[j]])  #the outside words (in windows)\n",
    "            t = t + 1\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "#    print(X.shape,Y.shape, 't=', t)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'the', 'deduction', 'of', 'the', 'costs', 'of', 'investing', 'beating', 'the', 'stock', 'market', 'is', 'a', \"loser's\", 'game']\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "#doc = \"After the deduction of the costs\" \n",
    "\n",
    "tokens = tokenize(doc)\n",
    "print(tokens)\n",
    "print()\n",
    "\n",
    "#import nltk\n",
    "#tokens = nltk.word_tokenize(doc)\n",
    "#print(tokens)\n",
    "\n",
    "print([i for i in range(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loser's\": 0, 'stock': 1, 'market': 3, 'beating': 2, 'costs': 4, 'of': 5, 'after': 6, 'the': 7, 'deduction': 8, 'a': 9, 'investing': 10, 'is': 11, 'game': 12}\n",
      "\n",
      "{0: \"loser's\", 1: 'stock', 2: 'beating', 3: 'market', 4: 'costs', 5: 'of', 6: 'after', 7: 'the', 8: 'deduction', 9: 'a', 10: 'investing', 11: 'is', 12: 'game'}\n"
     ]
    }
   ],
   "source": [
    "word_to_id, id_to_word = mapping(tokens)\n",
    "print(word_to_id)\n",
    "print()\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30) (1, 30)\n",
      "[ 7  6  8  7  5  8  7  5  4  7  5  4 10  5  2 10  7  2  1  7  3  1 11  3\n",
      "  9 11  0  9 12  0]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 1\n",
    "X, Y = generate_training_data(tokens, word_to_id, window_size)\n",
    "print(X.shape,Y.shape)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1\n",
    "\n",
    "print(Y.flatten())\n",
    "\n",
    "print(Y_one_hot.astype(int)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2]\n",
      "[0 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((3,3))\n",
    "b=[2,2,2]\n",
    "b=np.array(b)\n",
    "print(b.flatten())\n",
    "print(np.arange(3))\n",
    "a[b.flatten(),np.arange(3)]=1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 16\n",
      "[]\n",
      "[1, 2, 3]\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "N = len(tokens)\n",
    "print('N =',N)\n",
    "nbr_inds = list(range(max(0, i - 3), i)) + list(range(i + 1, min(N, i + 3 + 1)))\n",
    "print(list(range(max(0, i - 3), i)))\n",
    "print(list(range(i + 1, min(N, i + 3 + 1))))\n",
    "print(nbr_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30) [[ 6  7  7  8  8  5  5  7  7  4  4  5  5 10 10  2  2  7  7  1  1  3  3 11\n",
      "  11  9  9  0  0 12]]\n",
      "\n",
      "(1, 30) [[ 7  6  8  7  5  8  7  5  4  7  5  4 10  5  2 10  7  2  1  7  3  1 11  3\n",
      "   9 11  0  9 12  0]]\n",
      "(13, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,X)\n",
    "print()\n",
    "print(Y.shape,Y)\n",
    "print(Y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    initialize all the trianing parameters\n",
    "    \"\"\"\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00273695 -0.00494449  0.00817263]\n",
      " [-0.00205551 -0.0055717  -0.00473694]\n",
      " [-0.01143236  0.00482378  0.00092643]\n",
      " [ 0.00542296 -0.01913017  0.00971176]\n",
      " [ 0.01895795  0.00517051 -0.01105913]\n",
      " [ 0.0072756  -0.0049198  -0.01168428]\n",
      " [-0.00346664 -0.00532906  0.01203248]\n",
      " [-0.00046539  0.0194206   0.00227028]\n",
      " [-0.00027444  0.01185689 -0.00737148]\n",
      " [ 0.00459522 -0.0020205  -0.01155637]\n",
      " [ 0.00834348  0.00158722 -0.00495168]\n",
      " [ 0.00573648  0.00836717  0.01135202]\n",
      " [ 0.00183763 -0.02149874  0.00974983]]\n",
      "[[ 2.43272299e-03 -1.35478807e-03  9.20326530e-03]\n",
      " [-1.46032421e-02  1.72006540e-02  8.02709283e-03]\n",
      " [ 2.22381606e-02 -4.21840274e-03  1.55464387e-03]\n",
      " [-6.69747123e-03  2.20050292e-02  2.75063957e-03]\n",
      " [ 6.56206921e-03  8.68914402e-03 -4.57766065e-03]\n",
      " [-3.88922049e-03 -2.73560117e-03 -7.32916089e-03]\n",
      " [ 4.97082619e-04  1.26166799e-02  1.19434427e-02]\n",
      " [ 2.17634713e-03  9.87780373e-04  1.14959767e-02]\n",
      " [ 1.03126429e-02 -3.36350746e-03  2.79422485e-03]\n",
      " [-3.24321882e-03 -9.47475532e-03 -3.30712744e-02]\n",
      " [-3.64116472e-05 -6.02164727e-03 -2.89239427e-03]\n",
      " [ 1.25738861e-02 -3.29101086e-03 -5.09963293e-03]\n",
      " [ 1.34712623e-03  4.33467298e-03  7.82806520e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(initialize_wrd_emb(13,3))\n",
    "print(initialize_dense(3,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
