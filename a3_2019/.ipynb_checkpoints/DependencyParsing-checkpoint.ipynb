{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224n Assignment #3: Dependency Parsing\n",
    "In this assignment, you will build a neural dependency parser using PyTorch. In Part 1, you will learn\n",
    "about two general neural network techniques (Adam Optimization and Dropout) that you will use to build\n",
    "the dependency parser in Part 2. In Part 2, you will implement and train the dependency parser, before\n",
    "analyzing a few erroneous dependency parses.\n",
    "### 1. Machine Learning & Neural Networks (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) (4 points) Adam Optimizer <br>\n",
    "Recall the standard Stochastic Gradient Descent update rule:\n",
    "$$  \\pmb{\\theta} ← \\pmb{\\theta} − \n",
    "\\alpha\\nabla_{\\boldsymbol{\\theta}}J_{minibatch}(\\pmb{\\theta}) $$\n",
    "\n",
    "where $\\pmb{\\theta}$ is a vector containing all of the model parameters, \n",
    "$J$ is the loss function, $∇_\\theta J_{minibatch}(\\pmb{\\theta})$ is the \n",
    "gradient of the loss function with respect to the parameters on a \n",
    "minibatch of data, and $\\alpha$ is the learning rate. Adam \n",
    "Optimization<sup>1</sup> uses a more sophisticated update rule with two \n",
    "additional\n",
    "steps<sup>2</sup>   <br>\n",
    "<font size=\"2\" color=\"grey\"> <sup>1</sup> Kingma and Ba, 2015, https://arxiv.org/pdf/1412.6980.pdf  </font><br>\n",
    "<font size=\"2\" color=\"grey\"> <sup>2</sup> The actual Adam update uses a few additional tricks that are less important, but we won’t worry about them here. </font>\n",
    "\n",
    "i. (2 points) First, Adam uses a trick called momentum by keeping track \n",
    "of **m**, a rolling averageof the gradients:\n",
    "$$\\textbf{m} ← \\beta_1\\textbf{m} + (1 − \\beta_1)∇_\\boldsymbol{\\theta} \n",
    "J_{minibatch}(\\pmb{\\theta}) $$\n",
    "$$ \\pmb{\\theta} ← \\pmb{\\theta} − \\alpha \\textbf{m} $$\n",
    "where $β_1$ is a hyperparameter between 0 and 1 (often set to 0.9). \n",
    "Briefly explain (you don’t need to prove mathematically, just give an \n",
    "intuition) how using **m** stops the updates from varying\n",
    "as much and why this low variance may be helpful to learning, overall. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=\"green\">Solution </font>\n",
    "\n",
    "<font color=\"green\">\n",
    "* The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions\n",
    "* Gain faster convergence and reduced oscillation\n",
    "\n",
    "* Imaging that the shape of loss function $J$ likes a bowl in 3-D space. We could consider the gradient of $J$ wrt $\\theta$, is a 'force'(also could be acceleration) push the loss function downward the slope of the bowl, and the $\\textbf{m}$ is a 'velocity' controls the speed and direction. With the help of acceleration, the loss function $J$ could be more faster to convergence, however, the factor(hyperparameter) $\\beta_1$ could be considered as a friction force preventing $J$ from overshooting. \n",
    "* The low variance could help $J$ reduce the vibration on the verical direction while downward to the convergence point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. (2 points) Adam also uses adaptive learning rates by keeping track \n",
    "of **v**, a rolling average of the magnitudes of the gradients:\n",
    "\n",
    "$$\n",
    "\\textbf{m} ← \\beta_1\\textbf{m} + (1 − \\beta_1)∇_{\\theta} \n",
    "J_{minibatch}(\\pmb{\\theta}) \\\\\n",
    "\\pmb{v} ← \\beta_2\\pmb{v} + (1 − \\beta_2)(∇_{\\theta} \n",
    "J_{minibatch}(\\pmb{\\theta}) \\odot ∇_{\\theta} J_{minibatch}(\\pmb{\\theta})) \\\\\n",
    "\\pmb{\\theta} ← \\pmb{\\theta} − \\alpha \\odot \\textbf{m}/\\sqrt{\\pmb{v}}\n",
    "$$\n",
    "\n",
    "where $\\odot$ and / denote elementwise multiplication and division (so \n",
    "$\\textbf{z}\\odot\\textbf{z}$ is elementwise squaring)\n",
    "and $\\beta_2$ is a hyperparameter between 0 and 1 (often set to 0.99). \n",
    "Since Adam divides the update\n",
    "by $\\sqrt{v}$, which of the model parameters will get larger updates? \n",
    "Why might this help with\n",
    "learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=\"green\">Solution: Adaptive learning rate: </font>\n",
    "<font color=\"green\">\n",
    "Used to normalize the parameter update step, element wise\n",
    "Weights that receive high gradients will have their effective learning rate reduced\n",
    "Weights that receive small / infrequent updates will have effective learning rate increased\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) (4 points) Dropout <sup>3</sup> \n",
    "<font size=\"2\" color=\"grey\"> <sup>3</sup> Srivastava et al., 2014, https://www.cs.toronto.edu/˜hinton/absps/JMLRdropout.pdf </font>\n",
    "\n",
    "Dropout is a regularization technique. During training, dropout randomly sets units in the hidden layer **h** to zero with probability pdrop (dropping different units each minibatch), and then multiplies **h** by a constant $γ$. We can write this as\n",
    "$$ h_{drop} = γ \\textbf{d} ◦ \\textbf{h} $$\n",
    "where $\\textbf{d} ∈ \\{0, 1\\}^ {D_h}$ ($D_h$ is the size of **h**) is a mask vector where each entry is 0 with probability $p_{drop}$ and 1 with probability (1 − $p_{drop}$). $γ$ is chosen such that the expected value of **h**_{drop} is **h**:\n",
    "$$\\textbf{E}_{p_{drop}} [\\textbf{h}_{drop}]_i = h_i $$\n",
    "for all $i ∈ \\{1, . . . , D_h\\}$.\n",
    "\n",
    "i. (2 points) What must $γ$ equal in terms of $p_{drop}$? Briefly justify your answer. <br>\n",
    "ii. (2 points) Why should we apply dropout during training but not during evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=\"green\">Solution: Dropout </font>\n",
    "\n",
    "<font color=\"green\">\n",
    "$\\mathrm{i.}$ We know that for a matrix $A$, the expectation is: \n",
    "    $$\\mathbb{E}[\\mathbf{A}]_i = \\mathbb{E}[\\mathbf{A}_i]$$   \n",
    "\n",
    "So, the expected value of $\\mathbf{h}_{drop}$ is:\n",
    "$$\\begin{aligned} \\mathbb{E}_{p_{drop}}[\\mathbf{h}_{drop}]_i &= \\mathbb{E}_{p_{drop}}[\\gamma \\mathbf{d} \\circ \\mathbf{h}]_i \\ &= \\gamma \\mathbb{E}{p_{drop}}[d_i h_i] \\ &= \\gamma 0.8h_i \\end{aligned} $$\n",
    "\n",
    "  Now we make $\\gamma 0.8h_i = h_i$, so $\\gamma = 1.25$\n",
    "\n",
    "$\\mathrm{ii.}$ If we apply dropout during evaluation, we'll get a random(uncertain) result for the keep_prob, that's bad for evaluation.\n",
    "\n",
    "* $\\gamma = 1/p$ for scaling output on training (Not sure)  <br>\n",
    "* Dropout in training but not in testing :\n",
    "    * At test time all neurons see all their inputs, so we want the outputs of neurons at test time to be identical to their expected outputs at training time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Transition-Based Dependency Parsing (42 points)\n",
    "In this section, you’ll be implementing a neural-network based dependency parser, with the goal of maximizing performance on the UAS (Unlabeled Attachemnt Score) metric.\n",
    "\n",
    "Before you begin please install PyTorch 1.0.0 from https://pytorch.org/get-started/locally/\n",
    "with the CUDA option set to None. Additionally run pip install tqdm to install the tqdm package\n",
    "– which produces progress bar visualizations throughout your training process.\n",
    "\n",
    "A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between\n",
    "head words, and words which modify those heads. Your implementation will be a transition-based parser,\n",
    "which incrementally builds up a parse one step at a time. At every step it maintains a partial parse,\n",
    "which is represented as follows:\n",
    "\n",
    "* A stack of words that are currently being processed.\n",
    "* A buffer of words yet to be processed.\n",
    "* A list of dependencies predicted by the parser.\n",
    "\n",
    "Initially, the stack only contains ROOT, the dependencies list is empty, and the buffer contains all words\n",
    "of the sentence in order. At each step, the parser applies a transition to the partial parse until its buffer\n",
    "is empty and the stack size is 1. The following transitions can be applied:\n",
    "\n",
    "* SHIFT: removes the first word from the buffer and pushes it onto the stack.\n",
    "* LEFT-ARC: marks the second (second most recently added) item on the stack as a dependent of\n",
    "the first item and removes the second item from the stack.\n",
    "* RIGHT-ARC: marks the first (most recently added) item on the stack as a dependent of the second\n",
    "item and removes the first item from the stack.\n",
    "\n",
    "On each step, your parser will decide among the three transitions using a neural network classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) (6 points) Go through the sequence of transitions needed for parsing the sentence “I parsed this sentence correctly”. The dependency tree for the sentence is shown below. At each step, give the configuration of the stack and buffer, as well as what transition was applied this step and what new dependency was added (if any). The first three steps are provided below as an example.\n",
    "<img src=\"a3dependency.png\" style=\"width: 900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=\"green\">Solution  </font>\n",
    "\\begin{array}{l|l|l|l} \n",
    "Stack & Buffer & New dependency & Transition \\\\ \n",
    "\\hline \n",
    "[Root] & [I, parsed,this, sentence, correctly] &   & Initial \\space Configuration \\\\\n",
    "[Root, I] & [parsed,this, sentence, correctly] &   & SHIFT \\\\\n",
    "[Root, I, parsed] & [this, sentence, correctly]   &   & SHIFT \\\\\n",
    "[Root, parsed] & [this, sentence, correctly] & parsed \\rightarrow I & LEFT-ARC \\\\\n",
    "[Root, parsed, this] & [sentence, correctly] & & SHIFT \\\\\n",
    "[Root, parsed, this, sentence] & [correctly] & & SHIFT \\\\\n",
    "[Root, parsed, sentence] & [correctly] & sentence \\rightarrow this & LEFT-ARC \\\\\n",
    "[Root, parsed] & [correctly] & parsed \\rightarrow sentence & RIGHT-ARC \\\\\n",
    "[Root, parsed, correctly] & [] & & SHIFT \\\\\n",
    "[Root, parsed] & [] & parsed \\rightarrow correctly & RIGHT-ARC \\\\ \n",
    "[Root] & [] & Root \\rightarrow parsed & RIGHT-ARC \n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (2 points) A sentence containing n words will be parsed in how many steps (in terms of n)? Briefly\n",
    "explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=\"green\">Solution:  </font>\n",
    "<font color=\"green\">\n",
    "A sentence contain n words will be parsed in 2n+1 steps:\n",
    "* 1 step of Initial Configuration\n",
    "* n \"SHIFT\" operations to read n words into stack\n",
    "* n \"ARC\" operations to remove word from stack\n",
    "* Total steps: 1(Initial) + n (SHIFT) + n (ARC) = 2n + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) (6 points) Implement the **init** and parse step functions in the **PartialParse** class in\n",
    "parser **transitions.py**. This implements the transition mechanics your parser will use. You\n",
    "can run basic (non-exhaustive) tests by running  **  python parser_transitions.py  part_c**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CS224N 2018-19: Homework 3\n",
    "parser_model.py: Feed-Forward Neural Network for Dependency Parsing\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\"\"\"\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ParserModel(nn.Module):\n",
    "    \"\"\" Feedforward neural network with an embedding layer and single hidden layer.\n",
    "    The ParserModel will predict which transition should be applied to a\n",
    "    given partial parse configuration.\n",
    "\n",
    "    PyTorch Notes:\n",
    "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
    "            are a subclass of this \"nn.Module\".\n",
    "        - The \"__init__\" method is where you define all the layers and their respective parameters\n",
    "            (embedding layers, linear layers, dropout layers, etc.).\n",
    "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
    "            when you write \"m = ParserModel()\".\n",
    "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
    "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
    "            in other ParserModel methods.\n",
    "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, n_features=36,hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
    "        \"\"\" \n",
    "        Initialize the parser model.\n",
    "        @param embeddings (Tensor): word embeddings (num_words, embedding_size)\n",
    "        @param n_features (int): number of input features\n",
    "        @param hidden_size (int): number of hidden units\n",
    "        @param n_classes (int): number of output classes\n",
    "        @param dropout_prob (float): dropout probability\n",
    "        \"\"\"\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
    "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "\n",
    "        ### YOUR CODE HERE (~5 Lines)\n",
    "        ### TODO:\n",
    "        ###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix\n",
    "        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
    "        ###     2) Construct `self.dropout` layer.\n",
    "        ###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix\n",
    "        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
    "        ###\n",
    "        ### Note: Here, we use Xavier Uniform Initialization for our Weight initialization.\n",
    "        ###         It has been shown empirically, that this provides better initial weights\n",
    "        ###         for training networks than random uniform initialization.\n",
    "        ###         For more details checkout this great blogpost:\n",
    "        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization \n",
    "        ### Hints:\n",
    "        ###     - After you create a linear layer you can access the weight\n",
    "        ###       matrix via:\n",
    "        ###         linear_layer.weight\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
    "        ###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_\n",
    "        ###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
    "        self.embed_to_hidden = nn.linear(self.embed_size*self.n_feature,self.hidden_size)\n",
    "        nn.init.xavier_uniform(self.embed_to_hidden.weight, gain=1)\n",
    "        self.dropout = nn.Dropout( p = self.dropout_prob )\n",
    "        self.hidden_to_logits = nn.linear(self.hidden_size, self.n_feature)\n",
    "        nn.init.xavier_uniform(self.hidden_to_logits.weight, gain = 1)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def embedding_lookup(self, t):\n",
    "        \"\"\" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)\n",
    "            to embedding vectors.\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__\n",
    "                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).\n",
    "                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to\n",
    "                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)\n",
    "                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.\n",
    "\n",
    "            @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "            @return x (Tensor): tensor of embeddings for words represented in t\n",
    "                                (batch_size, n_features * embed_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~1-3 Lines)\n",
    "        ### TODO:\n",
    "        ###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.\n",
    "        ###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).\n",
    "        ###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)\n",
    "        ###\n",
    "        ### Note: In order to get batch_size, you may need use the tensor .size() function:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\n",
    "        ###\n",
    "        ###  Please see the following docs for support:\n",
    "        ###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "        \n",
    "        x = self.pretrained_embeddings ( t )\n",
    "        x = x.view(x.size()[0],-1)  #shape (batch_size, n_features*embedding_size) #the size -1 is inferred from other dimension\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\" Run the model forward.\n",
    "\n",
    "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
    "                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.\n",
    "                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,\n",
    "                    the `forward` function would called on `t` and the result would be stored in the `output` variable:\n",
    "                        model = ParserModel()\n",
    "                        output = model(t) # this calls the forward function\n",
    "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
    "\n",
    "        @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
    "                                 without applying softmax (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ###  YOUR CODE HERE (~3-5 lines)\n",
    "        ### TODO:\n",
    "        ###     1) Apply `self.embedding_lookup` to `t` to get the embeddings\n",
    "        ###     2) Apply `embed_to_hidden` linear layer to the embeddings\n",
    "        ###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.\n",
    "        ###     4) Apply dropout layer to the output of step 3.\n",
    "        ###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.\n",
    "        ###\n",
    "        ### Note: We do not apply the softmax to the logits here, because\n",
    "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
    "        \n",
    "        x = self.embedding_lookup( t )\n",
    "        x = self.embed_to_hidden ( x )\n",
    "        x = F.relu( x )                         #import torch.nn.functional as F\n",
    "        x = self.dropout( x )\n",
    "        logits = self.hidden_to_logits ( x )\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHIFT test passed!\n",
      "LEFT-ARC test passed!\n",
      "RIGHT-ARC test passed!\n",
      "parse test passed!\n",
      "minibatch_parse test passed!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CS224N 2018-19: Homework 3\n",
    "parser_transitions.py: Algorithms for completing partial parsess.\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"Initializes this partial parse.\n",
    "\n",
    "        @param sentence (list of str): The sentence to be parsed as a list of words.\n",
    "                                        Your code should not modify the sentence.\n",
    "        \"\"\"\n",
    "        # The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.\n",
    "        self.sentence = sentence\n",
    "\n",
    "        ### YOUR CODE HERE (3 Lines)\n",
    "        ### Your code should initialize the following fields:\n",
    "        ###     self.stack: The current stack represented as a list with the top of the stack as the\n",
    "        ###                 last element of the list.\n",
    "        ###     self.buffer: The current buffer represented as a list with the first item on the\n",
    "        ###                  buffer as the first item of the list\n",
    "        ###     self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
    "        ###             tuples where each tuple is of the form (head, dependent).\n",
    "        ###             Order for this list doesn't matter.\n",
    "        ###\n",
    "        ### Note: The root token should be represented with the string \"ROOT\"\n",
    "        ###\n",
    "        self.stack = ['ROOT']\n",
    "        self.buffer = list(sentence)\n",
    "        self.dependencies = []\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "\n",
    "    def parse_step(self, transition):\n",
    "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
    "\n",
    "        @param transition (str): A string that equals \"S\", \"LA\", or \"RA\" representing the shift,\n",
    "                                left-arc, and right-arc transitions. You can assume the provided\n",
    "                                transition is a legal transition.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~7-10 Lines)\n",
    "        ### TODO:\n",
    "        ###     Implement a single parsing step, i.e. the logic for the following as\n",
    "        ###     described in the pdf handout:\n",
    "        ###         1. Shift\n",
    "        ###         2. Left Arc\n",
    "        ###         3. Right Arc\n",
    "        if(transition == 'S') and len(self.buffer) > 0:\n",
    "            word = self.buffer.pop(0)\n",
    "            self.stack.append(word)\n",
    "        if(transition == 'LA'):\n",
    "            head = self.stack[-1]\n",
    "            dependent = self.stack.pop(-2)\n",
    "            self.dependencies.append((head,dependent))\n",
    "        if(transition == 'RA'):\n",
    "            head = self.stack[-2]\n",
    "            dependent = self.stack.pop()\n",
    "            self.dependencies.append((head,dependent))\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def parse(self, transitions):\n",
    "        \"\"\"Applies the provided transitions to this PartialParse\n",
    "\n",
    "        @param transitions (list of str): The list of transitions in the order they should be applied\n",
    "\n",
    "        @return dsependencies (list of string tuples): The list of dependencies produced when\n",
    "                                                        parsing the sentence. Represented as a list of\n",
    "                                                        tuples where each tuple is of the form (head, dependent).\n",
    "        \"\"\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dependencies\n",
    "\n",
    "\n",
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
    "\n",
    "    @param sentences (list of list of str): A list of sentences to be parsed\n",
    "                                            (each sentence is a list of words and each word is of type string)\n",
    "    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function\n",
    "                                model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
    "                                returns a list of transitions predicted for each parse. That is, after calling\n",
    "                                    transitions = model.predict(partial_parses)\n",
    "                                transitions[i] will be the next transition to apply to partial_parses[i].\n",
    "    @param batch_size (int): The number of PartialParses to include in each minibatch\n",
    "\n",
    "\n",
    "    @return dependencies (list of dependency lists): A list where each element is the dependencies\n",
    "                                                    list for a parsed sentence. Ordering should be the\n",
    "                                                    same as in sentences (i.e., dependencies[i] should\n",
    "                                                    contain the parse for sentences[i]).\n",
    "    \"\"\"\n",
    "    dependencies = []\n",
    "\n",
    "    ### YOUR CODE HERE (~8-10 Lines)\n",
    "    ### TODO:\n",
    "    ###     Implement the minibatch parse algorithm as described in the pdf handout\n",
    "    ###\n",
    "    ###     Note: A shallow copy (as denoted in the PDF) can be made with the \"=\" sign in python, e.g.\n",
    "    ###                 unfinished_parses = partial_parses[:].\n",
    "    ###             Here `unfinished_parses` is a shallow copy of `partial_parses`.\n",
    "    ###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances\n",
    "    ###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.\n",
    "    ###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`\n",
    "    ###             contains references to the same objects. Thus, you should NOT use the `del` operator\n",
    "    ###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that\n",
    "    ###             is being accessed by `partial_parses` and may cause your code to crash.\n",
    "    num_sentence = len(sentences)\n",
    "    partial_parses = [PartialParse(sentence) for sentence in sentences]\n",
    "    unfinished_parses = partial_parses[:] #shallow copy of partial_parses\n",
    "    \n",
    "    while len(unfinished_parses) > 0:\n",
    "        parsers = unfinished_parses[:batch_size]\n",
    "        batch_trasitions = model.predict(parsers)\n",
    "        for pp, transition in zip(parsers, batch_trasitions):\n",
    "            pp.parse([transition])\n",
    "            if len(pp.buffer) == 0 and len(pp.stack) == 1:\n",
    "                unfinished_parses.remove(pp)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    dependencies = [pp.dependencies for pp in partial_parses]\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "def test_step(name, transition, stack, buf, deps,\n",
    "              ex_stack, ex_buf, ex_deps):\n",
    "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
    "    pp = PartialParse([])\n",
    "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
    "\n",
    "    pp.parse_step(transition)\n",
    "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
    "    assert stack == ex_stack, \\\n",
    "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
    "    assert buf == ex_buf, \\\n",
    "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
    "    print(\"{:} test passed!\".format(name))\n",
    "\n",
    "\n",
    "def test_parse_step():\n",
    "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
    "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
    "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
    "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
    "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
    "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n",
    "\n",
    "\n",
    "def test_parse():\n",
    "    \"\"\"Simple tests for the PartialParse.parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
    "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
    "    dependencies = tuple(sorted(dependencies))\n",
    "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
    "    assert dependencies == expected,  \\\n",
    "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
    "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
    "        \"parse test failed: the input sentence should not be modified\"\n",
    "    print(\"parse test passed!\")\n",
    "\n",
    "\n",
    "class DummyModel(object):\n",
    "    \"\"\"Dummy model for testing the minibatch_parse function\n",
    "    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
    "    the sentence is \"right\", \"left\" if otherwise.\n",
    "    \"\"\"\n",
    "    def predict(self, partial_parses):\n",
    "        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
    "                for pp in partial_parses]\n",
    "\n",
    "\n",
    "def test_dependencies(name, deps, ex_deps):\n",
    "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
    "    deps = tuple(sorted(deps))\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
    "\n",
    "\n",
    "def test_minibatch_parse():\n",
    "    \"\"\"Simple tests for the minibatch_parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
    "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
    "                 [\"left\", \"arcs\", \"only\"],\n",
    "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
    "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
    "    test_dependencies(\"minibatch_parse\", deps[0],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[1],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[2],\n",
    "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[3],\n",
    "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
    "    print(\"minibatch_parse test passed!\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     args = sys.argv\n",
    "#     if len(args) != 2:\n",
    "#         raise Exception(\"You did not provide a valid keyword. Either provide 'part_c' or 'part_d', when executing this script\")\n",
    "#     elif args[1] == \"part_c\":\n",
    "#         test_parse_step()\n",
    "#         test_parse()\n",
    "#     elif args[1] == \"part_d\":\n",
    "#         test_minibatch_parse()\n",
    "#     else:\n",
    "#         raise Exception(\"You did not provide a valid keyword. Either provide 'part_c' or 'part_d', when executing this script\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_parse_step()\n",
    "    test_parse()\n",
    "    \n",
    "#    elif args[1] == \"part_d\":\n",
    "    test_minibatch_parse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before POP: [1, 8, 27, 64, 125, 216]\n",
      "After POP: [1, 8, 27, 64, 125]\n",
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "List = [1, 8, 27, 64, 125, 216]\n",
    "\n",
    "print(\"Before POP:\", List)\n",
    "\n",
    "List.pop(-1)\n",
    "\n",
    "print(\"After POP:\", List)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING\n",
      "================================================================================\n",
      "Loading data...\n",
      "took 1.32 seconds\n",
      "Building parser...\n",
      "took 0.76 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 2.09 seconds\n",
      "Vectorizing data...\n",
      "took 1.13 seconds\n",
      "Preprocessing training data...\n",
      "took 27.88 seconds\n",
      "took 0.02 seconds\n",
      "\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|█████████████████████████████████████████████████████▊                        | 1274/1848 [04:18<02:01,  4.72it/s]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CS224N 2018-19: Homework 3\n",
    "run.py: Run the dependency parser.\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "from parser_model import ParserModel\n",
    "from utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter\n",
    "\n",
    "# -----------------\n",
    "# Primary Functions\n",
    "# -----------------\n",
    "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
    "    \"\"\" Train the neural dependency parser.\n",
    "\n",
    "    @param parser (Parser): Neural Dependency Parser\n",
    "    @param train_data ():\n",
    "    @param dev_data ():\n",
    "    @param output_path (str): Path to which model weights and results are written.\n",
    "    @param batch_size (int): Number of examples in a single batch\n",
    "    @param n_epochs (int): Number of training epochs\n",
    "    @param lr (float): Learning rate\n",
    "    \"\"\"\n",
    "    best_dev_UAS = 0\n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE (~2-7 lines)\n",
    "    ### TODO:\n",
    "    ###      1) Construct Adam Optimizer in variable `optimizer`\n",
    "    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func`\n",
    "    ###\n",
    "    ### Hint: Use `parser.model.parameters()` to pass optimizer\n",
    "    ###       necessary parameters to tune.\n",
    "    ### Please see the following docs for support:\n",
    "    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n",
    "    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
    "        if dev_UAS > best_dev_UAS:\n",
    "            best_dev_UAS = dev_UAS\n",
    "            print(\"New best dev UAS! Saving model.\")\n",
    "            torch.save(parser.model.state_dict(), output_path)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
    "    \"\"\" Train the neural dependency parser for single epoch.\n",
    "\n",
    "    Note: In PyTorch we can signify train versus test and automatically have\n",
    "    the Dropout Layer applied and removed, accordingly, by specifying\n",
    "    whether we are training, `model.train()`, or evaluating, `model.eval()`\n",
    "\n",
    "    @param parser (Parser): Neural Dependency Parser\n",
    "    @param train_data ():\n",
    "    @param dev_data ():\n",
    "    @param optimizer (nn.Optimizer): Adam Optimizer\n",
    "    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n",
    "    @param batch_size (int): batch size\n",
    "    @param lr (float): learning rate\n",
    "\n",
    "    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n",
    "    \"\"\"\n",
    "    parser.model.train() # Places model in \"train\" mode, i.e. apply dropout layer\n",
    "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(n_minibatches)) as prog:\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
    "            optimizer.zero_grad()   # remove any baggage in the optimizer\n",
    "            loss = 0. # store loss for this batch here\n",
    "            train_x = torch.from_numpy(train_x).long()\n",
    "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n",
    "\n",
    "            ### YOUR CODE HERE (~5-10 lines)\n",
    "            ### TODO:\n",
    "            ###      1) Run train_x forward through model to produce `logits`\n",
    "            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n",
    "            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n",
    "            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n",
    "            ###         are the predictions (y^ from the PDF).\n",
    "            ###      3) Backprop losses\n",
    "            ###      4) Take step with the optimizer\n",
    "            ### Please see the following docs for support:\n",
    "            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n",
    "            logitss = parser.model.forward(train_x)\n",
    "\n",
    "#            optimizer.zero_grad()\n",
    "            loss = loss_func(logitss, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ### END YOUR CODE\n",
    "            prog.update(1)\n",
    "            loss_meter.update(loss.item())\n",
    "\n",
    "    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n",
    "\n",
    "    print(\"Evaluating on dev set\",)\n",
    "    parser.model.eval() # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
    "    dev_UAS, _ = parser.parse(dev_data)\n",
    "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "    return dev_UAS\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Note: Set debug to False, when training on entire corpus\n",
    "    # debug = True\n",
    "    debug = False\n",
    "\n",
    "    # assert(torch.__version__ == \"1.0.0\"),  \"Please install torch version 1.0.0\"\n",
    "\n",
    "    print(80 * \"=\")\n",
    "    print(\"INITIALIZING\")\n",
    "    print(80 * \"=\")\n",
    "    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n",
    "\n",
    "    start = time.time()\n",
    "    model = ParserModel(embeddings)\n",
    "    parser.model = model\n",
    "    print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "\n",
    "    print(80 * \"=\")\n",
    "    print(\"TRAINING\")\n",
    "    print(80 * \"=\")\n",
    "    output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
    "    output_path = output_dir + \"model.weights\"\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n",
    "\n",
    "    if not debug:\n",
    "        print(80 * \"=\")\n",
    "        print(\"TESTING\")\n",
    "        print(80 * \"=\")\n",
    "        print(\"Restoring the best model weights found on the dev set\")\n",
    "        parser.model.load_state_dict(torch.load(output_path))\n",
    "        print(\"Final evaluation on test set\",)\n",
    "        parser.model.eval()\n",
    "        UAS, dependencies = parser.parse(test_data)\n",
    "        print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
